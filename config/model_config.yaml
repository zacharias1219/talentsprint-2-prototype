# Model Training Configuration
training:
  base_model: "FinGPT/fingpt-forecaster_dow30_llama2-7b-lora"
  learning_rate: 2e-5
  batch_size: 8
  gradient_accumulation_steps: 4
  num_epochs: 5
  max_seq_length: 1024
  warmup_steps: 100
  weight_decay: 0.01
  optimizer: "AdamW"
  lr_scheduler: "cosine"
  save_steps: 500
  eval_steps: 250
  logging_steps: 50
  fp16: true
  gradient_checkpointing: true
  output_dir: "models/fine_tuned"

# LoRA Configuration
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

# Evaluation Configuration
evaluation:
  metrics:
    - "accuracy"
    - "financial_qa_score"
    - "domain_knowledge_score"
    - "hallucination_rate"
  benchmark_dataset: "data/processed/benchmark.json"
  expert_alignment_threshold: 0.8

# Inference Configuration
inference:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true

# Embedding Model Configuration
embedding:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
  device: "cuda"
  batch_size: 32

